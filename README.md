<div style="overflow-x: auto; max-width: 100%;">
<h1 align="center">üî≠
Identification des relations spatiales dans les images en utilisant les r√©seaux de neurones convolutifs.
</h1>
<p align="center">  
<a href="https://arxiv.org/abs/1908.02660">R√©f√©rence</a>
.
<a href="https://zenodo.org/records/8104370)">Dataset SpatialSense </a>

</p>

***
###   Introduction:
***

<p align="justify">
Les relations spatiales entre les objets fournissent des informations importantes pour les t√¢ches de compr√©hension et d‚Äôinterpr√©tation de sc√®nes. Ainsi, l'int√©gration de ces informations spatiales dans l'analyse d'images peut augmenter l'efficacit√© et la qualit√© de leur traitement.
La reconnaissance de relations spatiales est utilis√©e dans plusieurs domaines, tels que la d√©tection d'objets, la g√©n√©ration de l√©gendes, la cr√©ation de descripteurs pour les images, etc.<br>
De nombreuses approches dans la litt√©rature ont √©t√© pr√©sent√©es. On peut citer par exemple les descripteurs classiques relations spatiales, qui incluent les mod√®les topologiques (mod√®les bas√©es sur le calcul des connexions des r√©gions, RCC) et directionnels (histogramme des forces).
Ces mod√®les constituent des repr√©sentations condens√©es des donn√©es, ax√©es sur des aspects sp√©cifiques, tels que la topologie pour le mod√®le RCC8, la direction pour l‚Äôhistogramme d‚Äôangle, ou des caract√©ristiques hybrides, comme le R-histogramme qui combine les descripteurs topologiques et directionnels.
Cependant, les r√©centes performances des mod√®les d'apprentissage profond dans diverses taches (reconnaissance d'objets ,classification de sc√®nes, traitement automatique du langage) encouragent l‚Äôapplication de ces mod√®les dans l'apprentissage automatique des relations directement √† partir des images. Ces approches concernent les mod√®les d‚Äôattention textuels (BERT) et visuels (CNN).<br>
D'autres approches combinant ces mod√®les ont √©t√© √©tudi√©s (DRNet, VTransE). L‚Äôobjectif est d'exploit√© un ensemble d'informations li√©es √† la position des objets dans l'espace (les boites englobantes), leur apparence ainsi que leurs classes s√©mantiques,  pour pr√©dire des relations spatiales pr√©cises. <br>
L'objectif de notre travail est de classer les configurations spatiales. Pour ce faire, plusieurs mod√®les ont √©t√© impl√©ment√©s pour mod√©liser les relations spatiales dans les images. Le premier mod√®le est bas√© sur le r√©seau  VggNet pr√©entrain√© pour extraire les caract√©ristiques des images, suivie d‚Äôun perceptron multicouche (MLP) pour la classification des relations spatiale. La deuxi√®me approche est un MLP √† trois couches cach√©es entrain√© pour mod√©liser les configurations spatiales en fonction des coordonn√©es des boites englobantes des deux objets. La troisi√®me approche est une combinaison des deux premi√®res, et prend en compte pour la classification les informations visuelles (images) ainsi que la position des objets dans l‚Äôespace, d√©finie par les coordonn√©es des boites englobantes. La derni√®re approche est un mod√®le qui combine les informations visuelles (images), textuelles (description de l‚Äôimage) ainsi que les informations  relatives √† la position des objets (boites englobantes). Ces mod√®les sont entra√Æn√©s sur deux bases de donn√©es d‚Äôimages: SpatialSense et SimpleShapes.<br>
L‚Äôarticle est organis√© comme suit. Dans la section mat√©rielle et m√©thode, nous d√©crirons les donn√©es utilis√©es ainsi que les mod√®les d√©velopp√©s. Dans la section r√©sultats et discutions, nouspr√©senterons les r√©sultats des diff√©rents mod√®les utilis√©s.
</p>

***
###   1. Mat√©riels et m√©thodes:
***
* ### Les donn√©es:
#### Dataset SimpleShapes:

<p align="justify">
Deux dataset sont utilis√©s pour entra√Æner les mod√®les. Le premier dataset SimpleShapes (figure 1) corresponds √† un ensemble de 1508 images segment√©s de dimension (224 X 224).  Chaque image montre deux formes segment√©es, d√©cris par leurs boites englobantes. Les formes peuvent √™tre plus ou moins complexes. Ces donn√©es d√©crivent  4 classes (Above, Left, Right, Under). Cette base de donn√©es pr√©sente l‚Äôavantage d'√™tre √©quilibr√©, c‚Äôest-√†-dire que chaque classe est repartie de fa√ßon √©gale dans la base. De plus, pour chaque classe, les donn√©es sont repr√©sentatives de la diversit√© des objets de la base (chaque classe contient des images qui repr√©sentent tous les objets de la base). La description des relations spatiales dans cette base de donn√©es est d√©crite par un triplets (sujet, relation, objet).
   
</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/cf2022e5-1cb0-48ba-a246-51c76983aafa" alt="Figure 1: Images de la base de donn√©es SimpleShapes.">
</p>

#### Dataset SpatialSense:

<p align="justify">
SpatialSense est une base de donn√©es con√ßue pour √©valuer la compr√©hension de la configuration spatiale. Ce dataset est  construit gr√¢ce √† un crowdsourcing contradictoire, dans lequel des annotateurs humains sont charg√©s de trouver des relations spatiales difficiles √† pr√©dire. Chaque image est d√©crite par une phrase (vraie ou fausse) ainsi que les coordonn√©es des boites englobantes des objets d‚Äôinter√™ts. De la m√™me fa√ßon que la base SimpleShapes, la description des relations spatiales dans ce dataset est d√©crite aussi par un triplet (sujet, relation, objet).
Cette base de donn√©es pr√©sente de nombreux avantages, notamment en termes de vari√©t√© de sc√®ne (environnements urbains, paysages naturels, sc√®nes int√©rieures, etc.) et de diversit√© des √©l√©ments qui d√©crivent les sujets et les objets (figure 2). Cependant, cette base pr√©sente une limite majeur li√©es √† la complexit√© des relations spatiales. En effet, les relations spatiales repr√©sent√©es dans SpatialSense ne se limitent pas aux interactions simples (comme c‚Äôest le cas pour la base SimpleShapes) . Elles incluent des configurations complexes et ambigu√´s (figure 3), √† l‚Äôorigine de d√©fis suppl√©mentaires en termes de pr√©diction et d‚Äôinterpr√©tation des relations spatiales. Ces ambigu√Øt√©s sont dues √† la subjectivit√© de la perception spatiale (les relations spatiales telles que "gauche" et ¬†" droite " peuvent √™tre interpr√©t√© en fonction de l‚Äôobservateur), √† l‚Äôambigu√Øt√© des instructions de crowdsourcing (interpr√©tation sp√©cifique), √† la variabilit√© des points de vue (la perception des relations spatiales depend de l‚Äôemplacement de l‚Äôobservateur, de la prise en compte des perspectives dans les images, etc.), et √† l‚Äôerreur humaine. Nous verrons  par la suite que toutes ces ambigu√Øt√©s dans les donn√©es de SpatialSense ont une influence sur la performance des mod√®les.

</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/9e212699-2fd3-4113-ba8e-a5708c5f11b7" alt="Figure 2: Images de la base de donn√©es SpatialSense qui ne pr√©sentent pas d‚Äô ambigu√Øt√©s.">
</p>

* ### Les mod√®les:
<p align="justify">
4 mod√®les ont √©t√© d√©velopp√©s pour int√©grer la perception spatiale dans les images. Ces mod√®les peuvent prendre en compte uniquement l‚Äôinformation extraite des images (image) ou les informations  relatives √† la position des objets (boites englobantes) ,ou la combinaison de ces informations (images + boites englobantes). Le dernier mod√®le qui sera pr√©sent√© inclura en plus de ces √©l√©ments l'information textuelle.
</p>

#### Mod√®les de classification d‚Äôimages (mod√®le 1): 
<p align="justify">
Le mod√®le utilis√© est d√©velopp√© pour classifier les images en fonction de la position relative des objets d‚Äôint√©r√™t. Ce mod√®le prend en entr√©e des images RGB ou des images segment√©es (les √©l√©ments segment√©s correspondent au sujet et  l‚Äôobjet). <br>
L'architecture du mod√®le pr√©sent√© dans la figure suivante (figure 4) se compose de deux √©tapes principales : le pr√©traitement des entr√©es et la g√©n√©ration du mod√®le. Dans la phase de pr√©traitement, les caract√©ristiques des images sont extraites en utilisant le mod√®le VGGNet pr√©-entrain√©, modifi√© pour √™tre enti√®rement convolutif. Ensuite,  un perceptron multicouche (MLP) √† deux couches cach√©s est entra√Æn√© pour d√©tecter les relations spatiales √† partir des vecteurs de caract√©ristiques des images . 
La premi√®re couche du MLP (FC-0) r√©duit la dimensionalit√© √† 512, et la seconde couches  (FC-1) la r√©duit davantage √† 256. La derni√®re couche du mod√®le correspondant a une couche de sortie avec 4 neurones (correspondant au nombre de classes), caract√©ris√©e par la fonction d‚Äôactivation softmax.
</p>
<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/4d3e322e-3633-4d83-92e3-26a47868d131" alt="Figure 4: Mod√®le d‚Äôapprentissage des relations spatiales entre les objets dans une image (mod√®le 1: CNN pr√©-entra√Æn√© + MLP)">
</p>

#### Mod√®les de classification des boites englobantes (mod√®le 2):
<p align="justify">
   
   L‚Äôarchitecture du mod√®le correspond √† un MLP √† 3 couches cach√©es (figure 5). La couche d‚Äôentr√©e prend une liste de 16 √©l√©ments, qui correspondent aux coordonn√©es des boites englobantes respectivement pour le sujet et l‚Äôobjet. Les 3 couches cach√©es sont compos√© respectivement de 64, 32 et 16 neurones et utilisent la fonction d'activation ReLU. La derni√®re couche du r√©seau est une couche dense avec un nombre de neurones √©quivalent au nombre de classes (4) que le mod√®le doit pr√©dire. Elle utilise la fonction d'activation softmax.
</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/89130972-98d9-48c2-b8bc-430b1ff45065" alt="Figure 5: Mod√®le de classification des coordonn√©es des bo√Ætes englobantes (mod√®le 2, MLP)">
</p>

#### Mod√®le de combinaison (mod√®le 3):
<p align="justify">
Ce mod√®le prend en entr√©e le mod√®le de classification d‚Äôimages ainsi que le mod√®le de classification des boites englobantes vues pr√©c√©demment (figure 6) . Les vecteurs de caract√©ristiques extraites des images et des boites englobantes sont ensuite concat√©n√©s. Ainsi, les informations issues des deux mod√®les sont r√©unies pour obtenir un mod√®le de classification plus puissant. La prise de d√©cision se fait par la derni√®re couche dense qui est ajout√© pour permettre la classification.
</p>
<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/b088b47f-b5b5-41da-8af1-a871274b88c2" alt="Figure 6: Mod√®le de classification combinant  le mod√®le de classification des images et des bo√Ætes englobantes (mod√®le3).">
</p>

#### Mod√®le multimodal (mod√®le 4):
<p align="justify">
Ce mod√®le prend en entr√©e un ensemble d‚Äôimages, des coordonn√©es des boites englobantes ainsi qu‚Äôune description textuelle des relations entres les objets d'int√©r√™t dans l‚Äôimage (figure 7).
Dans ce mod√®le, les repr√©sentations des mots-cl√©s des phases qui d√©crivent les images sont utilis√©es pour r√©aliser la classification des repr√©sentations spatiales. Pour extraire les repr√©sentations,  le mod√®le de langage XLM-Roberta est utilis√© pour  obtenir les repr√©sentations vectorielles des mots (embeddings) des phrases.
</p>
<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/4a8870f7-b374-4a6a-8a77-b13274701119" alt="Figure 7: Mod√®le s√©mantique de classification (mod√®le 4).">
</p>

* ### Les hyperparam√®tres:

<p align="justify">
Pour les 3 premiers mod√®les pr√©sent√©s , la recherche des hyperparam√®tres a √©t√© r√©aliser en utilisant la m√©thode  Grid Search coupl√©e a la k-fold cross validation. Cette m√©thode est d√©finie pour rechercher la valeur du taux d‚Äôapprentissage qui maximise la vitesse de convergence du mod√®le.
Elle √©value ensuite chaque combinaison par validation crois√©e pour d√©terminer celle qui offre les meilleures performances en termes d‚Äôaccuracy.
Le graphique suivant (figure 8) repr√©sente les r√©sultats d'une recherche d'hyperparam√®tres utilisant une m√©thode de type Grid Search. Chaque point correspond √†  un taux d'apprentissage diff√©rent.
Tout d‚Äôabord, on constate que les valeurs de  learning rate associ√©es √† une convergence rapide du mod√®le sont les valeurs comprises entre 10^-1 et 10^-4. Parmi ces valeurs, la valeur de learning rate qui maximise l‚Äôaccuracy est √©gale a 10^-3 (accuracy : 0.75). <br>

Voici un tableau r√©capitulatif des param√®tres des diff√©rents mod√®les (figure 9).

</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/664968c5-d61e-4e36-a258-4aa42730f522" alt="Figure 8: R√©sultat de la recherche de l‚Äôhyperparam√®tre maximisant la vitesse de convergence du mod√®le de classifications d‚Äôimages (modele 1).">
</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/cdd201ca-61f7-44f3-bacf-23dc4717163f" alt="Figure 9: Hyperparam√®tres des mod√®les.">
</p>

***
###   2. R√©sultats et discutions:
***

* ### R√©sultats des performances sur le dataset SimpleShapes:
<p align="justify">
Les r√©sultats des performances des mod√®les sur la base SimpleShapes sont pr√©sent√©s ci dessous (figure 10). Les performances des diff√©rents mod√®les sont √©valu√©s par l‚Äôaccuracy. <br>
Le mod√®le 1, qui traite les images, montre une convergence rapide (au bout de 6 √©poques) et une pr√©cision √©lev√©e ( 0.95). Ce mod√®le ne pr√©sente pas de trace de surapprentissage, comme en t√©moignent les courbes de pr√©cision d'entra√Ænement et de validation proches de l'une de l‚Äôautre.
 Le mod√®le 2, qui utilise les bo√Ætes englobantes, prend plus de temps pour converger et a une pr√©cision moins √©lev√©e. En effet, le mod√®le converge au bout de 45 √©poques pour une accuracy de 0.9. Cette difference dans le temps de convergence peut √™tre du √†  la profondeur du mod√®le.
Le mod√®le 3, combinant images et Bbox, semble offrir des performances similaires au mod√®le de classification des boites englobantes. Ces r√©sultats indiquent que le mod√®le qui prend en entr√©e uniquement les images est le plus performant.
   
</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/98147e39-8fbe-45ae-99ba-1de72cd19c4c" alt="Figure 10: Performances des mod√®les.">
</p>

* ### R√©sultats des performances sur le dataset SpatialSense:
<p align="justify">
Nous avons √©valu√© les performances du mod√®le de classification des images (mod√®les 1) sur les donn√©es de SpatialSense. 
D‚Äôapr√®s le graphique qui pr√©sente la pr√©cision d'entra√Ænement et de validation en fonction des √©poques, on constate que la pr√©cision d'entra√Ænement augmente de mani√®re constante, atteignant presque 1, tandis que la pr√©cision de validation semble atteindre 0,5. Cette diff√©rence entre la courbe des donn√©es d'entra√Ænement et de validation sugg√®re un surajustement du mod√®le (figure 11). <br>
Les performances du mod√®le sur un ensemble de donn√©es test sont pr√©sent√©es par la matrice de confusion (figure 11). D‚Äôapr√®s les r√©sultats de la matrice de confusion, on constate que les classes ‚Äògauche‚Äô et ¬†‚Äòen dessous de‚Äô ont les taux de classification les plus √©lev√©s, tandis que la classe ‚Äòdroite‚Äô a le taux le plus bas, et pr√©sente une confusion avec la classe ‚Äògauche‚Äô. <br> 
Les faibles performances du mod√®le ainsi que la confusion entre la classe   ‚Äòdroite‚Äô et ‚Äògauche‚Äô (donn√©es test) sont du aux ambigu√Øt√©s de la base SpatialSense. En effet, les performances du mod√®le de classification d‚Äôimages (mod√®le 1) entra√Æn√© par les sur les donn√©es SimpleShapes montrent de tr√®s bons r√©sultat.
  </p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/cb2ea653-d65f-4401-8660-572343db130a" alt="Figure 11: Performances du  mod√®le de classification d‚Äôimages (mod√®le 1) sur les donn√©es SpatialSense.">
</p>

<p align="justify">
Nous souhaitons savoir si un mod√®le qui prends en compte plusieurs types  d‚Äôinformations (mod√®le 3) peut capturer les relations complexes de la base de donn√©es  SpatialSense. Nous testons donc les performances du mod√®le 3 qui prends en compte les caract√©ristiques des images et des bo√Ætes englobantes en entrainant d‚Äôabord ce mod√®le sur les donn√©es  SpatialSense. Ces performances sont compar√©es au mod√®le pr√©-entra√Æn√© avec les donn√©es SimpleShapes et affin√© avec les donn√©es SpatialSense (figure 12). <br>
Les r√©sultats pr√©sent√©s montrent la comparaison de l‚Äôaccuracy de deux mod√®les de classification d‚Äôimages  (le mod√®le entra√Æn√© de z√©ro, et le  mod√®le pr√©-entra√Æn√© sur SimpleShapes, affin√© ensuite sur SpatialSense). Le mod√®le pr√©-entra√Æn√© et affin√© surpasse le mod√®le entra√Æn√© de z√©ro, comme on le voit par des courbes d'exactitude plus √©lev√©es pour les ensembles d'entra√Ænement et de validation.  Les matrices de confusion normalis√©es montrent que le mod√®le affin√© a des performances > 50 % pour l‚Äôensemble des 4 classes. De plus, on constate que l‚Äôambigu√Øt√© entre les classe ‚Äòdroite‚Äô et ‚Äògauches‚Äô semble moins visibles pour le mod√®le pr√©-entra√Æn√© sur SimpleShapes  et affin√© SpatialSense. En effet, le mod√®le affin√© pr√©dit correctement la classe ‚Äôleft‚Äô dans 52% des cas et pr√©dit correctement la classe ‚Äòright‚Äô dans 60% des cas.  √Ä l‚Äôinverse, le mod√®le entra√Æn√© de z√©ro confond pour tous les image test la classe ‚Äòleft‚Äô et ‚Äòright‚Äô. Ainsi, l‚Äôaffinement du mod√®le permet de mieux distinguer les classes ambigu√´s ‚Äòleft‚Äô et ‚Äòright‚Äô.
</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/b4f5881c-6a77-490e-aea0-506351e2f75d" alt="Figure 12">
</p>

<p align="justify">
Nous avons aussi √©valu√© le mod√®le multimodal  qui int√®gre les caract√©ristiques des images, des boites englobantes ainsi que les caract√©ristiques des mots des phrases associ√©es aux images (figure 13). <br>
Les performances du mod√®le au cours de l‚Äôentra√Ænement augmentent pour les donn√©es d‚Äôentrainement (jusqu‚Äôa 0.7 ) et de validation (jusqu‚Äôa 0.4). C‚Äôest observation indique que le mod√®le apprend, mais semble souffrir de surajustement, puisque l'accuracy sur les donn√©es validation est significativement inf√©rieure √† l'accuracy sur les donn√©es d‚Äôentra√Ænement. <br>
La matrice de confusion, qui t√©moigne des performances du mod√®le sur un ensemble de test montre que le mod√®le pr√©dit uniquement  la classe  ‚Äòleft‚Äô, avec toujours une ambigu√Øt√© entre les classes ‚Äòleft‚Äô et ‚Äòright‚Äô. Ces observations sugg√®rent que la prise en compte des repr√©sentations des mots n‚Äôapporte pas d‚Äôam√©lioration par rapport au mod√®le plus simple (mod√®le image + Bbox).
</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/78f3357e-1fd9-46fc-97d3-7b4566576831" alt="Figure 13: Performances du  mod√®le qui int√®gre en plus la repr√©sentation des mots (mod√®le 4)">
</p>

***
###  Conclusion:
***
<p align="justify">
L‚Äôobjectif √©tait de r√©aliser la classification des configurations spatiales. Pour cela, les performances de  4 mod√®les ont √©t√© test√©es sur deux bases de donn√©es, SimpleShapes qui ne pr√©sente pas d'ambigu√Øt√© et SpatialSense qui pr√©sente des erreurs de lab√©lisation. <br> 
Pour le dataset SimpleShapes, tous les mod√®les montrent des performances √©lev√©es, indiquant un bon ajustement sans surapprentissage (figure 14). Cependant, pour la base de donn√©es SpatialSense, le mod√®le le plus performant reste le mod√®le qui prend en compte uniquement les caract√©ristiques des images pour la classification des relations spatiales. Cependant, l'ambigu√Øt√© entre les classe ‚Äògauche‚Äô et ‚Äòdroite‚Äô reste fortement apparente (figure 11). L‚Äôutilisation du mod√®le pr√©-entra√Æn√© sur les donn√©es SimpleShapes et affin√© sur les donn√©es SpatialSense permet de r√©duire cette ambigu√Øt√© (figure 12).
</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/6798e693-387b-41b9-9a33-4a285d247eb3" alt="Figure 14: Performances des mod√®les sur les donn√©es test">
</p>


</div>









