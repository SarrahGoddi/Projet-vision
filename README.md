
<h1 align="center">üî≠
Identification des relations spatiales dans les images en utilisant les r√©seaux de neurones convolutifs.
</h1>
<p align="center">  
<a href="https://arxiv.org/abs/1908.02660">R√©f√©rence</a>
.
<a href="https://zenodo.org/records/8104370)">Dataset SpatialSense </a>

</p>

***
###   Introduction:
***

<p align="justify">
Les relations spatiales entre les objets fournissent des informations importantes pour les t√¢ches de compr√©hension et d‚Äôinterpr√©tation de sc√®nes. Ainsi, l'int√©gration de ces informations spatiales dans l'analyse d'images peut augmenter l'efficacit√© et la qualit√© de leur traitement.
La reconnaissance de relations spatiales est utilis√©e dans plusieurs domaines, tels que la d√©tection d'objets, la g√©n√©ration de l√©gendes, la cr√©ation de descripteurs pour les images, etc.<br>
De nombreuses approches dans la litt√©rature ont √©t√© pr√©sent√©es. On peut citer par exemple les descripteurs classiques relations spatiales, qui incluent les mod√®les topologiques (mod√®les bas√©es sur le calcul des connexions des r√©gions, RCC) et directionnels (histogramme des forces).
Ces mod√®les constituent des repr√©sentations condens√©es des donn√©es, ax√©es sur des aspects sp√©cifiques, tels que la topologie pour le mod√®le RCC8, la direction pour l‚Äôhistogramme d‚Äôangle, ou des caract√©ristiques hybrides, comme le R-histogramme qui combine les descripteurs topologiques et directionnels.
Cependant, les r√©centes performances des mod√®les d'apprentissage profond dans diverses taches (reconnaissance d'objets ,classification de sc√®nes, traitement automatique du langage) encouragent l‚Äôapplication de ces mod√®les dans l'apprentissage automatique des relations directement √† partir des images. Ces approches concernent les mod√®les d‚Äôattention textuels (BERT) et visuels (CNN).<br>
D'autres approches combinant ces mod√®les ont √©t√© √©tudi√©s (DRNet, VTransE). L‚Äôobjectif est d'exploit√© un ensemble d'informations li√©es √† la position des objets dans l'espace (les boites englobantes), leur apparence ainsi que leurs classes s√©mantiques,  pour pr√©dire des relations spatiales pr√©cises. <br>
L'objectif de notre travail est de classer les configurations spatiales. Pour ce faire, plusieurs mod√®les ont √©t√© impl√©ment√©s pour mod√©liser les relations spatiales dans les images. Le premier mod√®le est bas√© sur le r√©seau  VggNet pr√©entrain√© pour extraire les caract√©ristiques des images, suivie d‚Äôun perceptron multicouche (MLP) pour la classification des relations spatiale. La deuxi√®me approche est un MLP √† trois couches cach√©es entrain√© pour mod√©liser les configurations spatiales en fonction des coordonn√©es des boites englobantes des deux objets. La troisi√®me approche est une combinaison des deux premi√®res, et prend en compte pour la classification les informations visuelles (images) ainsi que la position des objets dans l‚Äôespace, d√©finie par les coordonn√©es des boites englobantes. La derni√®re approche est un mod√®le qui combine les informations visuelles (images), textuelles (description de l‚Äôimage) ainsi que les informations  relatives √† la position des objets (boites englobantes). Ces mod√®les sont entra√Æn√©s sur deux bases de donn√©es d‚Äôimages: SpatialSense et SimpleShapes.<br>
L‚Äôarticle est organis√© comme suit. Dans la section mat√©rielle et m√©thode, nous d√©crirons les donn√©es utilis√©es ainsi que les mod√®les d√©velopp√©s. Dans la section r√©sultats et discutions, nouspr√©senterons les r√©sultats des diff√©rents mod√®les utilis√©s.
</p>

***
###   1. Mat√©riels et m√©thodes:
***
* ### Les donn√©es:
#### Dataset SimpleShapes:

<p align="justify">
Deux dataset sont utilis√©s pour entra√Æner les mod√®les. Le premier dataset SimpleShapes (figure 1) corresponds √† un ensemble de 1508 images segment√©s de dimension (224 X 224).  Chaque image montre deux formes segment√©es, d√©cris par leurs boites englobantes. Les formes peuvent √™tre plus ou moins complexes. Ces donn√©es d√©crivent  4 classes (Above, Left, Right, Under). Cette base de donn√©es pr√©sente l‚Äôavantage d'√™tre √©quilibr√©, c‚Äôest-√†-dire que chaque classe est repartie de fa√ßon √©gale dans la base. De plus, pour chaque classe, les donn√©es sont repr√©sentatives de la diversit√© des objets de la base (chaque classe contient des images qui repr√©sentent tous les objets de la base). La description des relations spatiales dans cette base de donn√©es est d√©crite par un triplets (sujet, relation, objet).
   
</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/cf2022e5-1cb0-48ba-a246-51c76983aafa" alt="Figure 1: Images de la base de donn√©es SimpleShapes.">
</p>

#### Dataset SpatialSense:

<p align="justify">
SpatialSense est une base de donn√©es con√ßue pour √©valuer la compr√©hension de la configuration spatiale. Ce dataset est  construit gr√¢ce √† un crowdsourcing contradictoire, dans lequel des annotateurs humains sont charg√©s de trouver des relations spatiales difficiles √† pr√©dire. Chaque image est d√©crite par une phrase (vraie ou fausse) ainsi que les coordonn√©es des boites englobantes des objets d‚Äôinter√™ts. De la m√™me fa√ßon que la base SimpleShapes, la description des relations spatiales dans ce dataset est d√©crite aussi par un triplet (sujet, relation, objet).
Cette base de donn√©es pr√©sente de nombreux avantages, notamment en termes de vari√©t√© de sc√®ne (environnements urbains, paysages naturels, sc√®nes int√©rieures, etc.) et de diversit√© des √©l√©ments qui d√©crivent les sujets et les objets (figure 2). Cependant, cette base pr√©sente une limite majeur li√©es √† la complexit√© des relations spatiales. En effet, les relations spatiales repr√©sent√©es dans SpatialSense ne se limitent pas aux interactions simples (comme c‚Äôest le cas pour la base SimpleShapes) . Elles incluent des configurations complexes et ambigu√´s (figure 3), √† l‚Äôorigine de d√©fis suppl√©mentaires en termes de pr√©diction et d‚Äôinterpr√©tation des relations spatiales. Ces ambigu√Øt√©s sont dues √† la subjectivit√© de la perception spatiale (les relations spatiales telles que "gauche" et ¬†" droite " peuvent √™tre interpr√©t√© en fonction de l‚Äôobservateur), √† l‚Äôambigu√Øt√© des instructions de crowdsourcing (interpr√©tation sp√©cifique), √† la variabilit√© des points de vue (la perception des relations spatiales depend de l‚Äôemplacement de l‚Äôobservateur, de la prise en compte des perspectives dans les images, etc.), et √† l‚Äôerreur humaine. Nous verrons  par la suite que toutes ces ambigu√Øt√©s dans les donn√©es de SpatialSense ont une influence sur la performance des mod√®les.

</p>

<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/9e212699-2fd3-4113-ba8e-a5708c5f11b7" alt="Figure 2: Images de la base de donn√©es SpatialSense qui ne pr√©sentent pas d‚Äô ambigu√Øt√©s.">
</p>

* ### Les mod√®les:
<p align="justify">
4 mod√®les ont √©t√© d√©velopp√©s pour int√©grer la perception spatiale dans les images. Ces mod√®les peuvent prendre en compte uniquement l‚Äôinformation extraite des images (image) ou les informations  relatives √† la position des objets (boites englobantes) ,ou la combinaison de ces informations (images + boites englobantes). Le dernier mod√®le qui sera pr√©sent√© inclura en plus de ces √©l√©ments l'information textuelle.
</p>

#### Mod√®les de classification d‚Äôimages (mod√®le 1): 
<p align="justify">
Le mod√®le utilis√© est d√©velopp√© pour classifier les images en fonction de la position relative des objets d‚Äôint√©r√™t. Ce mod√®le prend en entr√©e des images RGB ou des images segment√©es (les √©l√©ments segment√©s correspondent au sujet et  l‚Äôobjet).
L'architecture du mod√®le pr√©sent√© dans la figure suivante (figure 4) se compose de deux √©tapes principales : le pr√©traitement des entr√©es et la g√©n√©ration du mod√®le. Dans la phase de pr√©traitement, les caract√©ristiques des images sont extraites en utilisant le mod√®le VGGNet pr√©-entrain√©, modifi√© pour √™tre enti√®rement convolutif. Ensuite,  un perceptron multicouche (MLP) √† deux couches cach√©s est entra√Æn√© pour d√©tecter les relations spatiales √† partir des vecteurs de caract√©ristiques des images . 
La premi√®re couche du MLP (FC-0) r√©duit la dimensionalit√© √† 512, et la seconde couches  (FC-1) la r√©duit davantage √† 256. La derni√®re couche du mod√®le correspondant a une couche de sortie avec 4 neurones (correspondant au nombre de classes), caract√©ris√©e par la fonction d‚Äôactivation softmax.
</p>
<p align="center">
  <img src="https://github.com/SarrahGoddi/Projet-vision/assets/157230807/4d3e322e-3633-4d83-92e3-26a47868d131" alt="Figure 4: Mod√®le d‚Äôapprentissage des relations spatiales entre les objets dans une image (mod√®le 1: CNN pr√©-entra√Æn√© + MLP)">
</p>




